{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import namedtuple, Counter\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tflearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 10  # number of partitions to split dataframe\n",
    "num_cores = 4  # number of cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaned(content):\n",
    "    # First remove inline JavaScript/CSS:\n",
    "    cleaned_content = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", content)\n",
    "    # Then remove html comments.\n",
    "    cleaned_content = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned_content)\n",
    "    # Next remove the remaining tags:\n",
    "    cleaned_content = re.sub(r\"(?s)<.*?>\", \" \", cleaned_content)\n",
    "    # Finally deal with whitespace\n",
    "    cleaned_content = re.sub(r\"&nbsp;\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"^$\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(\"''|,\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\" {2}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'s\", \" 's\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'m\", \" 'm\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ve\", \" 've\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"n\\'t\", \" n't\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'re\", \" 're\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'d\", \" 'd\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ll\", \" 'll\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\",\", \" , \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"!\", \" ! \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\(\", \" ( \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\)\", \" ) \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\?\", \" ? \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\s{2,}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\d+\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[\\r\\n]+\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r'^(https|http)?://.*[\\r\\n]*', '', cleaned_content)\n",
    "    return cleaned_content.strip()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def normalize(text, lowercase=True, remove_stopwords=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = nlp(text)\n",
    "    lemmatized = list()\n",
    "    for word in text:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "\n",
    "def cleanup_text(doc):\n",
    "    return normalize(cleaned(doc))\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanup_dataframe(data):\n",
    "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_df(data):\n",
    "    data['comment_text'] = data['comment_text'].apply(lambda x: tokenize(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_rare_words(df, min_count=4):\n",
    "    df = parallelize_dataframe(df, tokenize_df)\n",
    "    docs = df['comment_text']\n",
    "    word_cnt = Counter([w for doc in docs for w in doc])\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda doc: ' '.join([w for w in doc if word_cnt[w] >= min_count]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    # comment_text, labels...\n",
    "    df = pd.read_csv(os.path.join('data', filename))\n",
    "    df = parallelize_dataframe(df, cleanup_dataframe)\n",
    "    print('cleaned text data')\n",
    "    df = remove_rare_words(df, min_count=4)\n",
    "    print('removed rare words')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "Traceback (most recent call last):\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\", line 137, in lower\n",
      "    def lower(string): return string.lower()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\", line 137, in lower\n",
      "    def lower(string): return string.lower()\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/internals.py\", line 3212, in __setstate__\n",
      "    self._post_setstate()\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/internals.py\", line 3217, in _post_setstate\n",
      "    self._rebuild_blknos_and_blklocs()\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/internals.py\", line 3123, in _rebuild_blknos_and_blklocs\n",
      "    if (new_blknos == -1).any():\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/numpy/core/_methods.py\", line 38, in _any\n",
      "    return umr_any(a, axis, dtype, out, keepdims)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-f99f8291c3b8>\", line 1, in <module>\n",
      "    train_df = load_data('train.csv')\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 89, in load_data\n",
      "    df = parallelize_dataframe(df, cleanup_dataframe)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 62, in parallelize_dataframe\n",
      "    df = pd.concat(pool.map(func, df_split))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 260, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 602, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 599, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 666, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 712, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/bin/../lib/python3.6/posixpath.py\", line 386, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/bin/../lib/python3.6/posixpath.py\", line 420, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/bin/../lib/python3.6/posixpath.py\", line 169, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-6:\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in cleanup_dataframe\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/pandas/core/series.py\", line 2510, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 69, in <lambda>\n",
      "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 56, in cleanup_text\n",
      "    return normalize(cleaned(doc))\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"<ipython-input-4-95cfb361f6d5>\", line 41, in normalize\n",
      "    text = nlp(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 337, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/language.py\", line 365, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"tokenizer.pyx\", line 107, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\", line 137, in lower\n",
      "    def lower(string): return string.lower()\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"tokenizer.pyx\", line 161, in spacy.tokenizer.Tokenizer._tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"tokenizer.pyx\", line 240, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\", line 137, in lower\n",
      "    def lower(string): return string.lower()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"vocab.pyx\", line 134, in spacy.vocab.Vocab.get\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"vocab.pyx\", line 164, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\", line 137, in lower\n",
      "    def lower(string): return string.lower()\n",
      "  File \"/Users/blvp/Virtualenv/word-vec/lib/python3.6/site-packages/spacy/util.py\", line 322, in _get_attr_unless_lookup\n",
      "    def _get_attr_unless_lookup(default_func, lookups, string):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple('Dataset', ['x', 'y'])\n",
    "tf_idf = TfidfVectorizer(\n",
    "        tokenizer=tokenize,\n",
    "        preprocessor=None,\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        lowercase=True)\n",
    "documents = tf_idf.fit_transform(train_df['comment_text'])\n",
    "labels = train_df.drop(['id', 'comment_text'], axis=1)\n",
    "dataset = Dataset(documents.toarray(), np.array(labels))\n",
    "vocab_size = len(tf_idf.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 6\n",
    "\n",
    "input_data = tflearn.input_data([None, vocab_size])\n",
    "net = tflearn.fully_connected(input_data, n_classes, weights_init='xavier')\n",
    "net = tflearn.regression(net, loss='binary_crossentropy', n_classes=n_classes)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=1, tensorboard_dir='logs/experiment')\n",
    "print('model ready. start fitting the model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(dataset.x), dataset.y,\n",
    "          n_epoch=10,\n",
    "          validation_set=0.3,\n",
    "          show_metric=True,\n",
    "          batch_size=64,\n",
    "          shuffle=True,\n",
    "          run_id='my_example'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
