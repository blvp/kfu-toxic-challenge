{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import namedtuple, Counter\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tflearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 10  # number of partitions to split dataframe\n",
    "num_cores = 4  # number of cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaned(content):\n",
    "    # First remove inline JavaScript/CSS:\n",
    "    cleaned_content = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", content)\n",
    "    # Then remove html comments.\n",
    "    cleaned_content = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned_content)\n",
    "    # Next remove the remaining tags:\n",
    "    cleaned_content = re.sub(r\"(?s)<.*?>\", \" \", cleaned_content)\n",
    "    # Finally deal with whitespace\n",
    "    cleaned_content = re.sub(r\"&nbsp;\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"^$\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(\"''|,\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\" {2}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'s\", \" 's\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'m\", \" 'm\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ve\", \" 've\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"n\\'t\", \" n't\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'re\", \" 're\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'d\", \" 'd\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ll\", \" 'll\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\",\", \" , \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"!\", \" ! \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\(\", \" ( \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\)\", \" ) \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\?\", \" ? \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\s{2,}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\d+\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[\\r\\n]+\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r'^(https|http)?://.*[\\r\\n]*', '', cleaned_content)\n",
    "    return cleaned_content.strip()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def normalize(text, lowercase=True, remove_stopwords=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = nlp(text)\n",
    "    lemmatized = list()\n",
    "    for word in text:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "\n",
    "def cleanup_text(doc):\n",
    "    return normalize(cleaned(doc))\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanup_dataframe(data):\n",
    "    data['comment_text'] = data['comment_text'].apply(lambda x: cleanup_text(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_df(data):\n",
    "    data['comment_text'] = data['comment_text'].apply(lambda x: tokenize(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_rare_words(df, min_count=4):\n",
    "    df = parallelize_dataframe(df, tokenize_df)\n",
    "    docs = df['comment_text']\n",
    "    word_cnt = Counter([w for doc in docs for w in doc])\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda doc: ' '.join([w for w in doc if word_cnt[w] >= min_count]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    # comment_text, labels...\n",
    "    df = pd.read_csv(os.path.join('data', filename))\n",
    "    df = parallelize_dataframe(df, cleanup_dataframe)\n",
    "    print('cleaned text data')\n",
    "    df = remove_rare_words(df, min_count=4)\n",
    "    print('removed rare words')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple('Dataset', ['x', 'y'])\n",
    "tf_idf = TfidfVectorizer(\n",
    "        tokenizer=tokenize,\n",
    "        preprocessor=None,\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        lowercase=True)\n",
    "documents = tf_idf.fit_transform(train_df['comment_text'])\n",
    "labels = train_df.drop(['id', 'comment_text'], axis=1)\n",
    "dataset = Dataset(documents.toarray(), np.array(labels))\n",
    "vocab_size = len(tf_idf.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nclasses = 6\n",
    "\n",
    "input = tflearn.input_data([None, vocab_len])\n",
    "net = tflearn.fully_connected(input, n_classes, weights_init='xavier')\n",
    "net = tflearn.regression(net, loss='binary_crossentropy', n_classes=n_classes)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=1, tensorboard_dir='logs/experiment')\n",
    "print('model ready. start fitting the model')\n",
    "model.fit(dataset.x, dataset.y,\n",
    "          n_epoch=10,\n",
    "          validation_set=0.3,\n",
    "          show_metric=True,\n",
    "          batch_size=64,\n",
    "          shuffle=True,\n",
    "          run_id='bow_logits'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
