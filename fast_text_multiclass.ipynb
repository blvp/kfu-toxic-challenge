{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "from script import load_data\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(content):\n",
    "    # First remove inline JavaScript/CSS:\n",
    "    cleaned_content = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", content)\n",
    "    # Then remove html comments.\n",
    "    cleaned_content = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned_content)\n",
    "    # Next remove the remaining tags:\n",
    "    cleaned_content = re.sub(r\"(?s)<.*?>\", \" \", cleaned_content)\n",
    "    # Finally deal with whitespace\n",
    "    cleaned_content = re.sub(r\"&nbsp;\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"^$\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(\"''|,\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\" {2}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'s\", \" 's\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'m\", \" 'm\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ve\", \" 've\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"n\\'t\", \" n't\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'re\", \" 're\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'d\", \" 'd\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\'ll\", \" 'll\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\",\", \" , \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"!\", \" ! \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\(\", \" ( \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\)\", \" ) \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\?\", \" ? \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\s{2,}\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"\\d+\", \"\", cleaned_content)\n",
    "    cleaned_content = re.sub(r\"[\\r\\n]+\", \" \", cleaned_content)\n",
    "    cleaned_content = re.sub(r'^(https|http)?://.*[\\r\\n]*', '', cleaned_content)\n",
    "    return cleaned_content.strip()\n",
    "\n",
    "\n",
    "def to_fast_text_format(labels):\n",
    "    def _to_fast_text_format(item):\n",
    "        prepended = [\"__label__{}\".format(label) for label in labels if item[label] == 1]\n",
    "        if len(prepended) == 0:\n",
    "            prepended.append('__label__none')\n",
    "        return \" \".join(prepended)+ \" \" + str(item.comment_text)\n",
    "        \n",
    "    return _to_fast_text_format\n",
    "\n",
    "def load_and_save_ft_format(input_csv_file, output_ft_file):\n",
    "    labels = [\n",
    "        'toxic',\n",
    "        'obscene',\n",
    "        'insult',\n",
    "        'identity_hate',\n",
    "        'severe_toxic',\n",
    "        'threat'\n",
    "    ]\n",
    "    with open(input_csv_file, 'r') as csvfile:\n",
    "        df = pd.read_csv(input_csv_file)\n",
    "        out = df.apply(to_fast_text_format(labels), axis=1)\n",
    "        \n",
    "    with open(output_ft_file, 'w+') as ftfile:\n",
    "        for _, text in out.iteritems():\n",
    "            ftfile.write(text)\n",
    "            ftfile.write('\\n')\n",
    "    return len(out)\n",
    "\n",
    "def eval_model(input_file, ft, labels):\n",
    "    with open(input_file, 'r') as f:\n",
    "        ys = []\n",
    "        y_preds = []\n",
    "        for line in f.readlines():\n",
    "            sent = line.split()\n",
    "            sample_labels, sample = sent[:7], sent[7:]\n",
    "            sample = \" \".join(sample)\n",
    "            y = [1 if label in sample_labels else 0 for label in labels]\n",
    "            ys.append(y)\n",
    "            _, pred = ft.predict(sample, k=7)\n",
    "            y_preds.append(pred)\n",
    "        y_preds = np.array(y_preds)\n",
    "        ys = np.array(ys, dtype=np.float32)\n",
    "        y_preds = y_preds[:, 1:]\n",
    "        ys = ys[:, 1:]\n",
    "        print(input_file, \"auc: \", roc_auc_score(ys, y_preds, average=\"micro\"))\n",
    "\n",
    "def load_and_preprocess_data(data_type):\n",
    "    df = pd.read_csv(f'data/{data_type}.csv')\n",
    "    df['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: cleaned(x).lower())\n",
    "    df.to_csv(f'data/{data_type}.preprocessed.csv', index=False)\n",
    "    if data_type == 'train':\n",
    "        lino = load_and_save_ft_format(f'data/{data_type}.preprocessed.csv', f'data/{data_type}.fasttext.txt')\n",
    "        os.system(f'gshuf -o data/shuffled_{data_type}.txt data/{data_type}.fasttext.txt')\n",
    "        train_part = int(lino * 0.9)\n",
    "        dev_part = lino - train_part\n",
    "        os.system(f'head -n {train_part} data/shuffled_{data_type}.txt > data/fast_text_train')\n",
    "        os.system(f'tail -n {dev_part} data/shuffled_{data_type}.txt > data/fast_text_dev')\n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fast_text_dev auc:  0.8775110019227296\n",
      "data/fast_text_train auc:  0.8958360848684574\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    '__label__none',\n",
    "    '__label__toxic',\n",
    "    '__label__obscene',\n",
    "    '__label__insult',\n",
    "    '__label__identity_hate',\n",
    "    '__label__severe_toxic',\n",
    "    '__label__threat'\n",
    "]\n",
    "\n",
    "ft = fastText.train_supervised('data/fast_text_train',\n",
    "                               epoch=2, \n",
    "                               dim=100,\n",
    "                               lr=1.0,\n",
    "                               loss=\"softmax\",\n",
    "                               wordNgrams=2, \n",
    "                               verbose=2,\n",
    "                               minCount=1)\n",
    "ft.save_model('model.bin')\n",
    "eval_model('data/fast_text_dev', ft, labels)\n",
    "eval_model('data/fast_text_train', ft, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, comments, label_names):\n",
    "    out = []\n",
    "    target_order = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "    for row in comments.itertuples():\n",
    "        mapping, preds = model.predict(row.comment_text, k=len(label_names))\n",
    "        res = {label: pred for label, pred in zip(mapping, preds)}\n",
    "        prediction = [row.id]\n",
    "        for label in target_order:\n",
    "            ft_label = f'__label__{label}'\n",
    "            prediction.append(res[ft_label])\n",
    "        out.append(prediction)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data = load_and_preprocess_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = predictions(ft, submit_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.DataFrame(out, columns=['id','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "submit_df.to_csv('data/submition.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
